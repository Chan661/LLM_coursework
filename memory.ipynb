{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7731fa-1ab4-48cf-996c-a1a1fb6b2315",
   "metadata": {},
   "source": [
    "# Lec4. Adding Memory and Storage to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b0c75-2dd7-4662-b2ce-4e6b9208926c",
   "metadata": {},
   "source": [
    "Last week, we learned the basic elements of the framework LangChain. In this lecture, we are going to construct a vector store QA application from scratch.\n",
    "\n",
    ">Reference:\n",
    "> 1. [Ask A Book Questions](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/Ask%20A%20Book%20Questions.ipynb)\n",
    "> 2. [Agent Vectorstore](https://python.langchain.com/docs/modules/agents/how_to/agent_vectorstore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a649ab-bb72-4894-b526-c97a7aa1fd81",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34411a5b-ad45-4bf0-bf8e-91f71b256337",
   "metadata": {},
   "source": [
    "1. Install the requirements.  (Already installed in your image.)\n",
    "    ```\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "2. Get your OpenAI API; to get your Serpapi key, please sign up for a free account at the [Serpapi website](https://serpapi.com/); to get your Pinecone key, first regiter on the [Pinecone website](https://www.pinecone.io/), **Create API Key** and **Create Index**. Note that in this notebook the index's dimension should be 1536.\n",
    "\n",
    "3. Store your keys in a file named **.env** and place it in the current path or in a location that can be accessed.\n",
    "    ```\n",
    "    OPENAI_API_KEY='YOUR-OPENAI-API-KEY'\n",
    "    SERPAPI_API_KEY=\"YOUR-SERPAPI-API-KEY\"\n",
    "    PINECONE_API_KEY=\"YOUR-PINECONE-API-KEY\"\n",
    "    PINECONE_API_ENV=\"PINECONE-API-ENV\" # Should be something like \"gcp-starter\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defc9a3a-9f4c-49ff-8546-5799ff78b457",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb49c80a-4c12-4829-bef9-91076a4af689",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159215cd-4035-4f12-9904-e0ad32929b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5009a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function\n",
    "\n",
    "from pprint import pprint\n",
    "def print_with_type(res):\n",
    "    pprint(f\"%s:\" % type(res))\n",
    "    pprint(res)\n",
    "\n",
    "    pprint(f\"%s : %s\" % (type(res), res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d6985-1b36-4142-90b1-ad6386eb4335",
   "metadata": {},
   "source": [
    "## 1. Adding memory to remember the context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6809691-6aa5-4062-8d9e-1c620f9c6d2f",
   "metadata": {},
   "source": [
    "### 1.1 Use Conversation Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e8c81",
   "metadata": {},
   "source": [
    "#### Basic Use of ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b8fbfe-6cd7-4c4e-843f-92b0f4755d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<class 'dict'>:\"\n",
      "{'history': \"Human: hi\\nAI: what's up\"}\n",
      "'<class \\'dict\\'> : {\\'history\\': \"Human: hi\\\\nAI: what\\'s up\"}'\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Creating a memory and write to it.\n",
    "memory = ConversationBufferMemory()  # stores all histories as a single string\n",
    "memory.save_context({\"input\": \"hi\"}, \n",
    "                    {\"output\": \"what's up\"})\n",
    "print_with_type(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c34234",
   "metadata": {},
   "source": [
    "We can also get the history as a list of messages (this is useful if you are using this with a chat model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2794154-3b85-4012-b28f-2470cfeaa13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<class 'dict'>:\"\n",
      "{'history': [HumanMessage(content='hi'), AIMessage(content=\"what's up\")]}\n",
      "(\"<class 'dict'> : {'history': [HumanMessage(content='hi'), \"\n",
      " 'AIMessage(content=\"what\\'s up\")]}')\n"
     ]
    }
   ],
   "source": [
    "# get the history as a list of messages\n",
    "memory = ConversationBufferMemory(return_messages=True)  # stores messages as a list\n",
    "memory.save_context({\"input\": \"hi\"}, \n",
    "                    {\"output\": \"what's up\"})\n",
    "print_with_type(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7eafa",
   "metadata": {},
   "source": [
    "#### Managing Conversation Memory automatically in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e349aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edfe2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content = \"\"\"You are a chatbot having a conversation with a human. \n",
    "            Your name is Tom Marvolo Riddle. \n",
    "            You need to tell your name to that human if he doesn't know.\"\"\"\n",
    "        ),  # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name = \"chat_history\"\n",
    "        ),  # This is where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  # This is where the human input will be injected\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", \n",
    "                                  return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e2d0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set verbose as True to see more details\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory # Look at this line\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0db79f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human. \n",
      "            Your name is Tom Marvolo Riddle. \n",
      "            You need to tell your name to that human if he doesn't know.\n",
      "Human: Hi there, this is Harry Potter, I just got two good friends at Hogwarts, Ron Weasley and Hermione Granger.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello there, Harry Potter. I am Tom Marvolo Riddle. It's nice to meet you. Ron Weasley and Hermione Granger are indeed great friends to have at Hogwarts. How are you finding your time at the school so far?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm_chain.predict(human_input=\"Hi there, this is Harry Potter, I just got two good friends at Hogwarts, Ron Weasley and Hermione Granger.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e12e8428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Hi there, this is Harry Potter, I just got two good friends at Hogwarts, Ron Weasley and Hermione Granger.'),\n",
       "  AIMessage(content=\"Hello there, Harry Potter. I am Tom Marvolo Riddle. It's nice to meet you. Ron Weasley and Hermione Granger are indeed great friends to have at Hogwarts. How are you finding your time at the school so far?\")]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of messages in the memory \n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b0cf684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human. \n",
      "            Your name is Tom Marvolo Riddle. \n",
      "            You need to tell your name to that human if he doesn't know.\n",
      "Human: Hi there, this is Harry Potter, I just got two good friends at Hogwarts, Ron Weasley and Hermione Granger.\n",
      "AI: Hello there, Harry Potter. I am Tom Marvolo Riddle. It's nice to meet you. Ron Weasley and Hermione Granger are indeed great friends to have at Hogwarts. How are you finding your time at the school so far?\n",
      "Human: What are my best friends' names? \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your best friends' names are Ron Weasley and Hermione Granger.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm_chain.predict(human_input=\"What are my best friends' names? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ace5b2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Hi there, this is Harry Potter, I just got two good friends at Hogwarts, Ron Weasley and Hermione Granger.'),\n",
       "  AIMessage(content=\"Hello there, Harry Potter. I am Tom Marvolo Riddle. It's nice to meet you. Ron Weasley and Hermione Granger are indeed great friends to have at Hogwarts. How are you finding your time at the school so far?\"),\n",
       "  HumanMessage(content=\"What are my best friends' names? \"),\n",
       "  AIMessage(content=\"Your best friends' names are Ron Weasley and Hermione Granger.\")]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of messages in the memory \n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5184cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human. \n",
      "            Your name is Tom Marvolo Riddle. \n",
      "            You need to tell your name to that human if he doesn't know.\n",
      "Human: What are my best friends' names? \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello there! My name is Tom Marvolo Riddle. What are your best friends' names?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.clear()\n",
    "memory.load_memory_variables({})\n",
    "chat_llm_chain.predict(human_input=\"What are my best friends' names? \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451b9db-5213-463c-9867-5ecb67bd6d01",
   "metadata": {},
   "source": [
    "#### (Optional) Manipulate the memory by yourself in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "453b9ad2-45b7-4899-b28a-be80166d71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f429977-c439-4f56-bf42-191c14d6daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add memory to an arbitrary chain\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c28ebc5-9720-40e8-b8f6-497b14092367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<class 'langchain_core.messages.ai.AIMessage'>:\"\n",
      "AIMessage(content='Hello Harry! How can I assist you today?')\n",
      "(\"<class 'langchain_core.messages.ai.AIMessage'> : content='Hello Harry! How \"\n",
      " \"can I assist you today?'\")\n",
      "\"<class 'dict'>:\"\n",
      "{'history': []}\n",
      "\"<class 'dict'> : {'history': []}\"\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"Hi, I am Harry!\"}\n",
    "response = chain.invoke(inputs)\n",
    "print_with_type(response)\n",
    "print_with_type(memory.load_memory_variables({})) ##Looks like this kind does not store the memory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "205e3b28-4fd5-426a-87fd-4e88604762a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<class 'dict'>:\"\n",
      "{'history': [HumanMessage(content='Hi, I am Harry!'),\n",
      "             AIMessage(content='Hello Harry! How can I assist you today?')]}\n",
      "(\"<class 'dict'> : {'history': [HumanMessage(content='Hi, I am Harry!'), \"\n",
      " \"AIMessage(content='Hello Harry! How can I assist you today?')]}\")\n"
     ]
    }
   ],
   "source": [
    "# You need to save the context yourself\n",
    "memory.save_context(inputs, \n",
    "                    {\"output\": response.content})\n",
    "print_with_type(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12b115f9-c389-43a6-815a-351dfaea009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<class 'langchain_core.messages.ai.AIMessage'>:\"\n",
      "AIMessage(content='Your name is Harry!')\n",
      "\"<class 'langchain_core.messages.ai.AIMessage'> : content='Your name is Harry!'\"\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"What's my name?\"})\n",
    "print_with_type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc6973-7992-4e37-bda1-505c0e670df8",
   "metadata": {},
   "source": [
    "### 1.2 Using Entity memory\n",
    "\n",
    "#### Basic Use of ConversationEntityMemory\n",
    "\n",
    "Entity memory remembers given facts about specific entities in a conversation. It extracts information on entities (using an LLM) and builds up its knowledge about that entity over time (also using an LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76ae8f4b-067e-4a39-9802-8989513480cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76d147e6-bc48-4235-ba83-d40c83be86b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Harry & Ron are going to rescue a baby dragon in London.'),\n",
       "  AIMessage(content='That sounds like a great mission! What kind of mission are they working on?')],\n",
       " 'entities': {'Harry': 'Harry is going to rescue a baby dragon in London.',\n",
       "  'Ron': 'Ron is going to rescue a baby dragon in London with Harry.',\n",
       "  'Wei': '',\n",
       "  'London': 'London is the location where Harry and Ron are going to rescue a baby dragon.'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationEntityMemory(llm=llm, return_messages=True)\n",
    "inputs = {\"input\": \"Harry & Ron are going to rescue a baby dragon in London.\"}\n",
    "memory.load_memory_variables(inputs)\n",
    "memory.save_context(\n",
    "    inputs,\n",
    "    {\"output\": \"That sounds like a great mission! What kind of mission are they working on?\"}\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({\"input\": \"Harry and Ron and Wei and London?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aaf35a-8bbe-4274-9ed0-43d983e35e01",
   "metadata": {},
   "source": [
    "#### Using Entity in a chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3128de",
   "metadata": {},
   "source": [
    "Here we use ConversationChain.  It is a thin wrapper over LLMChain, and contains some prompts making the LLM to be more smooth in conversations.  See its source code for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9ac27a8-bfce-4066-b038-a75265036c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef6b28b9-4c78-4c7c-9164-89a60d980329",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5defa301-db7f-4bae-8b43-af49cee92f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Harry & Ron are going to rescue a baby dragon.',\n",
       " 'history': '',\n",
       " 'entities': {'Harry': '', 'Ron': ''},\n",
       " 'response': \" That sounds like quite an adventure! I hope they have a plan in place to safely rescue the dragon and return it to its natural habitat. Dragons can be quite dangerous, but I'm sure Harry and Ron are up for the challenge. Do you know where they are planning to rescue the dragon from?\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Harry & Ron are going to rescue a baby dragon.\") ##looks like new memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e492536-d6ee-4094-8bf5-4a17dd48348e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Harry': 'Harry is going on an adventure with Ron to rescue a baby dragon.',\n",
       " 'Ron': 'Ron is going on an adventure with Harry to rescue a baby dragon.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97567380-84a5-4517-be2f-5aef57b0f052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"They are trying to give the baby dragon to Ron's elder brother Charlie's friends and let them take the baby dragon to Romania.\",\n",
       " 'history': \"Human: Harry & Ron are going to rescue a baby dragon.\\nAI:  That sounds like quite an adventure! I hope they have a plan in place to safely rescue the dragon and return it to its natural habitat. Dragons can be quite dangerous, but I'm sure Harry and Ron are up for the challenge. Do you know where they are planning to rescue the dragon from?\",\n",
       " 'entities': {'Romania': ''},\n",
       " 'response': \" That's a great idea! Romania is known for its dragon reserves and I'm sure the baby dragon will be well taken care of there. I hope Harry and Ron are able to successfully deliver the dragon to Charlie's friends and ensure its safety.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"They are trying to give the baby dragon to Ron's elder brother Charlie's friends and let them take the baby dragon to Romania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62f5f13a-ff9f-407a-ae5f-1004ff8c0cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"Harry & Ron need to secretly transfer the baby dragon to the highest place in Hogwarts without let anyone see them. So they are going to use Harry's Invisibility cloak.\",\n",
       " 'history': \"Human: Harry & Ron are going to rescue a baby dragon.\\nAI:  That sounds like quite an adventure! I hope they have a plan in place to safely rescue the dragon and return it to its natural habitat. Dragons can be quite dangerous, but I'm sure Harry and Ron are up for the challenge. Do you know where they are planning to rescue the dragon from?\\nHuman: They are trying to give the baby dragon to Ron's elder brother Charlie's friends and let them take the baby dragon to Romania.\\nAI:  That's a great idea! Romania is known for its dragon reserves and I'm sure the baby dragon will be well taken care of there. I hope Harry and Ron are able to successfully deliver the dragon to Charlie's friends and ensure its safety.\",\n",
       " 'entities': {'Harry': 'Harry is going on an adventure with Ron to rescue a baby dragon.',\n",
       "  'Ron': 'Ron is going on an adventure with Harry to rescue a baby dragon.',\n",
       "  'Hogwarts': '',\n",
       "  \"Harry's Invisibility cloak\": ''},\n",
       " 'response': \" That's a clever plan! Harry's Invisibility cloak will definitely come in handy for this mission. I'm sure they will be able to successfully transfer the baby dragon without anyone noticing. Hogwarts is a big place, so finding the highest point might be a challenge, but I have faith in Harry and Ron's abilities.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Harry & Ron need to secretly transfer the baby dragon to the highest place in Hogwarts without let anyone see them. So they are going to use Harry's Invisibility cloak.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e27e48a2-862d-418b-9cdb-8fec5278d77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What do you know about Harry & Ron?',\n",
       " 'history': \"Human: Harry & Ron are going to rescue a baby dragon.\\nAI:  That sounds like quite an adventure! I hope they have a plan in place to safely rescue the dragon and return it to its natural habitat. Dragons can be quite dangerous, but I'm sure Harry and Ron are up for the challenge. Do you know where they are planning to rescue the dragon from?\\nHuman: They are trying to give the baby dragon to Ron's elder brother Charlie's friends and let them take the baby dragon to Romania.\\nAI:  That's a great idea! Romania is known for its dragon reserves and I'm sure the baby dragon will be well taken care of there. I hope Harry and Ron are able to successfully deliver the dragon to Charlie's friends and ensure its safety.\",\n",
       " 'entities': {'Harry': 'Harry is going on an adventure with Ron to rescue a baby dragon.',\n",
       "  'Ron': 'Ron is going on an adventure with Harry to rescue a baby dragon.'},\n",
       " 'response': ' Harry and Ron are two brave and adventurous friends who are on a mission to rescue a baby dragon. They are both skilled wizards and I have no doubt that they will be successful in their quest. They are also very loyal to each other and will do whatever it takes to protect the dragon and ensure its safety.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"What do you know about Harry & Ron?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66085a34-f6fd-4bb0-a91f-3b9aa96b278c",
   "metadata": {},
   "source": [
    "Now Let's inspect the entities that are extracted from the conversation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c36d9ec-2840-48db-b6a9-809229a53a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<class 'dict'>:\"\n",
      "{'Harry': 'Harry is using his Invisibility cloak to help transfer the baby '\n",
      "          'dragon to the highest point in Hogwarts without being seen.',\n",
      " \"Harry's Invisibility cloak\": 'Harry will use his Invisibility cloak to '\n",
      "                               'secretly transfer the baby dragon to the '\n",
      "                               'highest place in Hogwarts without being seen.',\n",
      " 'Hogwarts': 'Hogwarts is a big place, but Harry and Ron are planning to use '\n",
      "             \"Harry's Invisibility cloak to secretly transfer the baby dragon \"\n",
      "             'to the highest point without being seen.',\n",
      " 'Romania': 'Romania is known for its dragon reserves and is the planned '\n",
      "            \"destination for the baby dragon to be safely delivered to Ron's \"\n",
      "            \"elder brother Charlie's friends.\",\n",
      " 'Ron': 'Ron is helping Harry transfer the baby dragon to the highest place in '\n",
      "        \"Hogwarts using Harry's Invisibility cloak.\"}\n",
      "(\"<class 'dict'> : {'Harry': 'Harry is using his Invisibility cloak to help \"\n",
      " 'transfer the baby dragon to the highest point in Hogwarts without being '\n",
      " 'seen.\\', \\'Ron\\': \"Ron is helping Harry transfer the baby dragon to the '\n",
      " 'highest place in Hogwarts using Harry\\'s Invisibility cloak.\", \\'Romania\\': '\n",
      " '\"Romania is known for its dragon reserves and is the planned destination for '\n",
      " \"the baby dragon to be safely delivered to Ron's elder brother Charlie's \"\n",
      " 'friends.\", \\'Hogwarts\\': \"Hogwarts is a big place, but Harry and Ron are '\n",
      " \"planning to use Harry's Invisibility cloak to secretly transfer the baby \"\n",
      " 'dragon to the highest point without being seen.\", \"Harry\\'s Invisibility '\n",
      " 'cloak\": \\'Harry will use his Invisibility cloak to secretly transfer the '\n",
      " \"baby dragon to the highest place in Hogwarts without being seen.'}\")\n"
     ]
    }
   ],
   "source": [
    "print_with_type(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ad77c",
   "metadata": {},
   "source": [
    "Let's do more conversations and see what we can learn more about each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4821793a-fe46-4693-a940-18833f1b9a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes, Harry is definitely a brave and clever boy. He has proven time and time again that he is capable of handling difficult situations and coming up with creative solutions. I'm sure he will continue to impress us with his bravery and intelligence in the future.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Harry is a brave and clever boy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b340df6-b5c8-4731-8936-c8587eb1d527",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconversation\u001b[49m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mentity_store\u001b[38;5;241m.\u001b[39mstore\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conversation' is not defined"
     ]
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "718775f3-8188-4199-a663-72bd4b96eac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What do you know about Harry?',\n",
       " 'history': \"Human: They are trying to give the baby dragon to Ron's elder brother Charlie's friends and let them take the baby dragon to Romania.\\nAI:  That's a great idea! Romania is known for its dragon reserves and I'm sure the baby dragon will be well taken care of there. I hope Harry and Ron are able to successfully deliver the dragon to Charlie's friends and ensure its safety.\\nHuman: What do you know about Harry & Ron?\\nAI:  Harry and Ron are two brave and adventurous friends who are on a mission to rescue a baby dragon. They are both skilled wizards and I have no doubt that they will be successful in their quest. They are also very loyal to each other and will do whatever it takes to protect the dragon and ensure its safety.\\nHuman: Harry is a brave and clever boy.\\nAI:  Yes, Harry is definitely a brave and clever boy. He has proven himself time and time again in his adventures with Ron and I have no doubt that he will continue to do so in the future. His bravery and cleverness make him a valuable asset in any mission or quest.\",\n",
       " 'entities': {'Harry': 'Harry is a brave and clever boy who has proven himself time and time again in his adventures with Ron. His bravery and cleverness make him a valuable asset in any mission or quest.'},\n",
       " 'response': ' As an AI, I have access to a lot of information about Harry. From what I have gathered, he is a brave and clever boy who has proven himself time and time again in his adventures with Ron. His bravery and cleverness make him a valuable asset in any mission or quest.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"What do you know about Harry?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ab1e54-6c47-43ea-9750-af8840c1494d",
   "metadata": {},
   "source": [
    "### 1.3 Adding Memory to Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8bf04-34d1-4a86-82f8-de39be61b836",
   "metadata": {},
   "source": [
    "In this section, we will first ask the agent a question, and then without mention the context information ourselves ask another related question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edba9d7b-5baa-4769-9c2a-d11164941fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9088da52-e54b-468e-a158-9220360ea9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ee6d6dd-39eb-425f-bdda-8d5dc07e3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools,\n",
    "    prefix=\"\"\"Have a conversation with a human, answering the following questions as best you can.  You have access to the following tools:\"\"\",\n",
    "    suffix=\"\"\"Begin!  \n",
    "{chat_history}\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\",\n",
    "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0a5b87b-2c41-4b4a-b14a-cb30838c561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain.agents.mrkl.base.ZeroShotAgent` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use create_react_agent instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True, memory=memory, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed18da6e-6754-4b8b-8951-4bf916e3d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: I should use the Search tool to find the most recent data on China's population.\n",
      "Action: Search\n",
      "Action Input: \"China population 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe current population of China is 1,425,330,392 based on projections of the latest United Nations data. The UN estimates the July 1, 2024 population at 1,425,178,782.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is a projection and may not be the exact population in 2024, but it is the most recent and reliable estimate.\n",
      "Action: Search\n",
      "Action Input: \"China population 2024 estimate\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe current population of China is 1,425,292,075 as of Sunday, April 7, 2024, based on Worldometer elaboration of the latest United Nations data 1. China 2023 population is estimated at 1,425,671,352 people at mid year.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is a more specific and recent estimate, but it is still a projection and may not be the exact population in 2024.\n",
      "Action: Search\n",
      "Action Input: \"China population 2024 projection\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Download Table Data', 'source': 'World Population Review'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This website provides a table with the population of China from 1950 to 2020 and a projection for 2025. I can use this data to estimate the population in 2024.\n",
      "Action: Search\n",
      "Action Input: \"China population 2024 estimate from 1950 to 2020\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHistoric Population of China (1950 - 2024) Between 1950 and 2024 the population of China grown from 543,979,233 people to 1,425,178,782. An increase of the population size by 161.99% in 73 years.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This data shows a consistent increase in China's population over the years, so it is likely that the population in 2024 will be higher than the current estimate.\n",
      "Final Answer: Based on the data and projections, the population of China in 2024 is estimated to be around 1.4 to 1.5 billion people.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the population of China in 2024?',\n",
       " 'chat_history': '',\n",
       " 'output': 'Based on the data and projections, the population of China in 2024 is estimated to be around 1.4 to 1.5 billion people.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(input=\"What is the population of China in 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd9a011d-c1cc-4c1c-bf43-7b505eb97c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human: What is the population of China in 2024?\\nAI: Based on the data and projections, the population of China in 2024 is estimated to be around 1.4 to 1.5 billion people.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539886a-230d-497e-bcf3-1f60fc6d607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<class 'dict'>:\"\n",
      "{'chat_history': 'Human: What is the population of China in 2024?\\n'\n",
      "                 'AI: The estimated population of China in 2024 is '\n",
      "                 '1,425,178,782, with a projected growth rate of 0.1% '\n",
      "                 'annually. However, this is subject to change in the future.\\n'\n",
      "                 'Human: Is it more or less than India?\\n'\n",
      "                 'AI: In 2024, the population of China is projected to be '\n",
      "                 'slightly less than the population of India.'}\n"
     ]
    }
   ],
   "source": [
    "print_with_type(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff3ddfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: I should use the search tool to find the most recent and accurate information.\n",
      "Action: Search\n",
      "Action Input: \"population of China\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'population_result', 'place': 'China', 'population': '1.412 billion', 'year': '2022'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is a recent and reliable source, but I should double check with other sources to confirm the information.\n",
      "Action: Search\n",
      "Action Input: \"China population 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe current population of China is 1,425,330,392 based on projections of the latest United Nations data. The UN estimates the July 1, 2024 population at 1,425,178,782.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The population of China in 2024 is estimated to be around 1.4 to 1.5 billion people.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the population in Chima?',\n",
       " 'chat_history': 'Human: What is the population of China in 2024?\\nAI: Based on the data and projections, the population of China in 2024 is estimated to be around 1.4 to 1.5 billion people.',\n",
       " 'output': 'The population of China in 2024 is estimated to be around 1.4 to 1.5 billion people.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(input=\"what is the population in Chima?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e01167-55ca-4ba1-836d-c00e648e4634",
   "metadata": {},
   "source": [
    "## 2. Long term memory with vector storage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c1f37-7026-4d59-bf87-70c94eed7afa",
   "metadata": {},
   "source": [
    "In this section, we are going to embed the famous Harry Potter book's first chapter into a vectorstore and try some similarity searches. We have some extra examples commented, you can uncomment and try them one-by-one. If you observe the results carefully, you may find the characteristics of similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc75f3-1fd9-4076-8a3f-d809b7dbf572",
   "metadata": {},
   "source": [
    "### 2.1 Loaders and Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d598ad",
   "metadata": {},
   "source": [
    "#### PDF Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cac64c7-69dc-4450-b2fa-ffd05740411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, PDFMinerLoader\n",
    "\n",
    "data = PyPDFLoader(\"/share/lab4/hp-book1.pdf\").load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa40bc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>:\n"
     ]
    }
   ],
   "source": [
    "def print_with_type1(res):\n",
    "    print(f\"%s:\" % type(res))\n",
    "print_with_type1(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43a7e514-29b0-44ad-afbd-529f67296153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "\n",
    "#print (f'You have {len(data)} document(s) in your data')\n",
    "i = 0\n",
    "for d in data:\n",
    "    #print (f'There are {len(d.page_content)} characters in doc {i}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2396be0",
   "metadata": {},
   "source": [
    "#### Text file loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c50f2983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "union = TextLoader(\"/share/lab4/state_of_the_union.txt\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3caf81f",
   "metadata": {},
   "source": [
    "#### Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce5a0a",
   "metadata": {},
   "source": [
    "From Langchain documents: \n",
    "\n",
    "RecursiveCharacterTextSplitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e29fd56-3275-4041-ad1c-63f71a9d0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can have some trials with different chunk_size and chunk_overlap.\n",
    "# \n",
    "# \n",
    "#This is optional, test out on your own data.\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1a8d1fe-d24c-443d-bf68-43bdb0fc4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 135 documents\n"
     ]
    }
   ],
   "source": [
    "print (f'Now you have {len(texts)} documents')\n",
    "\n",
    "#for t in texts:\n",
    "   # print(t.page_content[:100])\n",
    "    #print(\"=========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515d70a",
   "metadata": {},
   "source": [
    "There are different kinds of splitters.  \n",
    "\n",
    "https://chunkviz.up.railway.app/ \n",
    "\n",
    "provides a great tool to see the splitter differences with different chunk_size and chunk_overlap settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a6a49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your TASK ####\n",
    "# Try different PDF Loaders.  Which one works the best for this file /share/lab4/hp-book1.pdf ,\n",
    "# which contains the full book of Harry Potter Book 1, with all the illustratons.\n",
    "\n",
    "## Langchain provides many other options for loaders, read the documents to find out the differences\n",
    "# See page https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
    "loader = UnstructuredPDFLoader(\"./data/field-guide-to-data-science.pdf\")\n",
    "# loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "# loader = PDFMinerLoader(\"example_data/layout-parser-paper.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c23cf-0d0a-4f79-9718-42d093fc4dd0",
   "metadata": {},
   "source": [
    "### 2.2 Create embeddings of your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1bbf6",
   "metadata": {},
   "source": [
    "Embedding is a model that turns a sentence into vectors, so that we can \"semantically search\" for related splits of a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd92c90b-7f21-4c8b-9586-41abf20b409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI embedding: slow and expensive, we do not use them here.  \n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ee572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Let's use the local ones.\n",
    "# We have downloaded a number of popular embedding models for you, in the /share/embedding directory, including\n",
    "# LaBSE\n",
    "# all-MiniLM-L12-v2\n",
    "# all-MiniLM-L6-v2\n",
    "# paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "minilm_embedding = SentenceTransformerEmbeddings(model_name=\"/share/embedding/all-MiniLM-L6-v2/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b805c79-c5b1-4812-9b85-671820ff0a69",
   "metadata": {},
   "source": [
    "### 2.4  Store and retrieve the embeddings in ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0c49f-541a-44da-8f5b-1e6059b478a6",
   "metadata": {},
   "source": [
    "You can search documents stored in \"Vector DBs\" by their semantic similarity.  Vector DBs uses an algorithm called \"KNN (k-nearest neighbors)\" to find documents whose embedding is the closest to the query. \n",
    "\n",
    "We first introduce ChromaDB becauase it runs locally, easy-to-set-up, and best of all, free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af187c9-e90d-40be-a74f-db620af2bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeddings and save the embeddings into ChromaDB\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_dir = \"/scratch2/chroma_db\"\n",
    "docsearch_chroma = Chroma.from_documents(data, \n",
    "                                         minilm_embedding,\n",
    "                                         persist_directory=chroma_dir,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "51465f59-49ed-45a5-832a-1f03b9560c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions from https://en.wikibooks.org/wiki/Muggles%27_Guide_to_Harry_Potter/Books/Philosopher%27s_Stone/Chapter_1\n",
    "# you can try yourself\n",
    "\n",
    "# query = 'Why would the Dursleys consider being related to the Potters a \"shameful secret\"?'\n",
    "# query = 'Who are the robed people Mr. Dursley sees in the streets?'\n",
    "# query = 'What might a \"Muggle\" be?'\n",
    "# query = 'What exactly is the cat on Privet Drive?'\n",
    "query = '''Who might \"You-Know-Who\" be? Why isn't this person referred to by a given name?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34cf7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A utiity function ...\n",
    "def print_search_results(docs):\n",
    "    print(f\"search returned %d results. \" % len(docs))\n",
    "    for doc in docs:\n",
    "        print(doc.page_content)\n",
    "        print(\"=============\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c24069d5-19d6-477b-a3ba-c79b6cd39d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic similarity search\n",
    "\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "#print_search_results(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a21737",
   "metadata": {},
   "source": [
    "#### Saving and Loading your ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "56bd7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local disk\n",
    "docsearch_chroma.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd053d53-d5a0-4e1f-8d56-69acb51d6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from disk\n",
    "docsearch_chroma_reloaded = Chroma(persist_directory = chroma_dir,\n",
    "                                   collection_name = 'harry-potter', \n",
    "                                   embedding_function = minilm_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ff1f853-d04e-4bae-8bf2-bddec2d4326e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docsearch_chroma_reloaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# you can test with the previous or another query\u001b[39;00m\n\u001b[1;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWho are the robed people Mr. Dursley sees in the streets?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mdocsearch_chroma_reloaded\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(query)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docsearch_chroma_reloaded' is not defined"
     ]
    }
   ],
   "source": [
    "# you can test with the previous or another query\n",
    "\n",
    "query = 'Who are the robed people Mr. Dursley sees in the streets?'\n",
    "docs = docsearch_chroma_reloaded.similarity_search(query)\n",
    "#print_search_results(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dd8cdb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search returned 4 results. \n",
      "wasn’t even Voldemort.\n",
      "=============\n",
      "mother died to save you. If there is  one thing Voldemort\n",
      "=============\n",
      "have found out somehow, this is  Voldemort we’re talking about,\n",
      "=============\n",
      "though he could, when I had Lord Voldemort on my side. . . .”\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "#### Your TASK ####\n",
    "# With the chosen PDF loaders, test different splitters and chunk size until you feel that the chuncking makes sense. \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0) #Never feel like make sense\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "docsearch_chroma = Chroma.from_documents(texts, \n",
    "                                         minilm_embedding, \n",
    "                                         collection_name='harry-potter', \n",
    "                                         persist_directory=chroma_dir,\n",
    "                                         )\n",
    "\n",
    "docs = docsearch_chroma.similarity_search('How did Voldemort die?')\n",
    "print_search_results(docs)\n",
    "\n",
    "docsearch_chroma.persist()\n",
    "\n",
    "docsearch_chroma_reloaded = Chroma(persist_directory = chroma_dir,\n",
    "                                   collection_name = 'harry-potter', \n",
    "                                   embedding_function = minilm_embedding)\n",
    "# You can also try different embeddings -> ask prof \n",
    "#Then embed the entire book 1 into ChormaDB ##file = hp-book1.pdf\n",
    "data = PyPDFLoader(\"/share/lab4/hp-book1.pdf\").load()\n",
    "texts = text_splitter.split_documents(data)\n",
    "docsearch_chroma = Chroma.from_documents(texts, \n",
    "                                         minilm_embedding, \n",
    "                                         collection_name='harry-potter', \n",
    "                                         persist_directory=chroma_dir,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa211d6-014a-4b99-8c79-1699557e0fe1",
   "metadata": {},
   "source": [
    "### 2.5 Query those docs with a QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4050ea43-6af4-4617-8c2e-c3b550336fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f035d818-b8cf-4d49-9a6f-ba233e204188",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9db75a21-9f02-4e09-98ef-81c40d6e04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did Harry's parents die?\"\n",
    "docs = docsearch_chroma_reloaded.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98270f41-fddd-4be0-909d-c80345e98470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "They're saying  he tried to kill the Potter's son, Harry. But -- he\n",
      "\n",
      "of it, he wasn't even sure his nephew was called Harry. He'd never even\n",
      "\n",
      "\"I've come to bring Harry to his aunt and uncle. They're the only family  \n",
      "he has left now.\"\n",
      "\n",
      "They're saying  he tried to kill the Potter's son, Harry. But -- he \n",
      "couldn't. He couldn't kill that little boy. No one knows why, or how,\n",
      "\n",
      "Question: How did Harry's parents die?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" It is not explicitly stated in the given context, but it can be inferred that Harry's parents died as a result of the attempted murder by the person mentioned in the context.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43d0d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your Task ####\n",
    "\n",
    "# Rebuild the chain from the whole book ChromaDB.  Test with one of the following questions (of your choice).\n",
    "\n",
    "query = 'Why does Dumbledore believe the celebrations may be premature?'\n",
    "#query = 'Why is Harry left with the Dursleys rather than a Wizard family?'\n",
    "#query = 'Why does McGonagall seem concerned about Harry being raised by the Dursleys?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4599ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your Task ####\n",
    "\n",
    "# Using langchain documentation, find out about the map reduce QA chain.  \n",
    "# answer the following questions using the chain\n",
    "chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "# answer one of the following questions of your choice. \n",
    "\n",
    "query = 'What happened in the Forbidden Forest during the first year of Harry Potter at Hogwarts?'\n",
    "# query = Tell me about Harry Potter and Quidditch during the first year\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0d396",
   "metadata": {},
   "source": [
    "### 2.6 Using Pinecone, an online vector DB\n",
    "\n",
    "You have many reasons to store your DB online in a SaaS / PaaS service.  For example, \n",
    "- you want to scale the queries to many concurrent users\n",
    "- you want more data reliability without having to worry about DB management\n",
    "- you want to share the DB but without owning any servers\n",
    "\n",
    "If you want to store your embeddings online, try pinecone with the code below. You must go to [Pinecone.io](https://www.pinecone.io/) and set up an account. Then you need to generate an api-key and create an \"index\", this can be done by navigating through the homepage once you've logged in to Pinecone, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b75f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# initialize pinecone, depends on two environment variables, os.environ['PINECONE_API_KEY'] and os.environ['PINECONE_API_ENV']\n",
    "pinecone.Pinecone()\n",
    "\n",
    "# You should create an index for your vector db.  \n",
    "# The \"dimension\" setting when you create the DB online, should be 1536 for openAI embedding, or 384 for minilm. \n",
    "index_name = \"lab1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f04c5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch_pinecone = Pinecone.from_texts(\n",
    "                                [ t.page_content for t in texts ], \n",
    "                                openai_embedding, \n",
    "                                index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba00d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "name of heaven did Ha rry survive?\"\n",
      "\n",
      "that Lily and James Potter are -- are -- that they're -- dead. \"\n",
      "\n",
      "\"The Potters, that's right, that's what I heard yes, their son, Harry\"\n",
      "\n",
      "-- an' poor little Harry off ter live with Muggles -\"\n",
      "\n",
      "Question: How did Harry's parents die?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "query = \"How did Harry's parents die?\"\n",
    "docs = docsearch_pinecone.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# we can use the full-book to test 'map-reduce'\n",
    "chain = load_qa_chain(llm, chain_type=\"map_reduce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7453fd84-ba39-4f2b-ab23-23b94d45d727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the sign that said Privet Drive -- no, looking at the sign; cats\n"
     ]
    }
   ],
   "source": [
    "# query with pinecone\n",
    "query = 'What exactly is the cat on Privet Drive?'\n",
    "docs = docsearch_pinecone.similarity_search(query)\n",
    "print(docs[0].page_content[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c1053e7a-3c56-44d2-9d87-1b08f624dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "name of heaven did Ha rry survive?\"\n",
      "\n",
      "that Lily and James Potter are -- are -- that they're -- dead. \"\n",
      "\n",
      "\"The Potters, that's right, that's what I heard yes, their son, Harry\"\n",
      "\n",
      "-- an' poor little Harry off ter live with Muggles -\"\n",
      "\n",
      "Question: How did Harry's parents die?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Your Task ####\n",
    "# modify the QA chain in Section 2.5 (Chapter 1 only) to use pinecone instead of ChromaDB\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "query = \"How did Harry's parents die?\"\n",
    "docs = docsearch_pinecone.similarity_search(query) ##Does not look like better?\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc6131-6a1a-40f5-8af0-afc5c723e49e",
   "metadata": {},
   "source": [
    "### 2.7 Use vector store in Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1053e7a-3c56-44d2-9d87-1b08f624dc53",
   "metadata": {},
   "source": [
    "In this section, we are going to create a simple QA agent that can decide by itself which of the two vectorstores it should switch to for questions of differnent fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b4118-4e34-4d3d-8230-a13bf77daa59",
   "metadata": {},
   "source": [
    "#### Preparing the tools for the agent.\n",
    "\n",
    "We will use our chroma_based Harry Potter vectorDB, and let's create another one containing President Biden's State of the Union speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "949662aa-5044-4899-ba50-5e06ac7df371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "documents = TextLoader('/share/lab4/state_of_the_union.txt').load()\n",
    "texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(documents)\n",
    "docsearch3 = Chroma.from_documents(texts, \n",
    "                                   minilm_embedding, \n",
    "                                   collection_name=\"state-of-union\", \n",
    "                                   persist_directory=\"/scratch2/chroma_db\")\n",
    "docsearch3.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b4118-4e34-4d3d-8230-a13bf77daa59",
   "metadata": {},
   "source": [
    "To allow the agent query these databases, we need to define two RetrievalQA chains. (Very Cool!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ccd41e19-fcff-4358-9374-2b36b29d1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "harry_potter = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                           chain_type=\"stuff\", \n",
    "                                           retriever=docsearch_chroma_reloaded.as_retriever())\n",
    "state_of_union = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                             chain_type=\"stuff\", \n",
    "                                             retriever=docsearch3.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dbe451ef-4137-4b86-9254-4117c6802b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<class 'dict'>:\"\n",
      "{'query': 'Why does McGonagall seem concerned about Harry being raised by the '\n",
      "          'Dursleys?',\n",
      " 'result': ' McGonagall is concerned because she knows that the Dursleys do '\n",
      "           'not like magic and have a negative attitude towards it. She '\n",
      "           'worries that Harry will not be raised in a loving and accepting '\n",
      "           'environment for his magical abilities.'}\n",
      "(\"<class 'dict'> : {'query': 'Why does McGonagall seem concerned about Harry \"\n",
      " \"being raised by the Dursleys?', 'result': ' McGonagall is concerned because \"\n",
      " 'she knows that the Dursleys do not like magic and have a negative attitude '\n",
      " 'towards it. She worries that Harry will not be raised in a loving and '\n",
      " \"accepting environment for his magical abilities.'}\")\n",
      "\"<class 'dict'>:\"\n",
      "{'query': 'what is the GDP increase last year?',\n",
      " 'result': ' The GDP increase last year was 5.7%.'}\n",
      "(\"<class 'dict'> : {'query': 'what is the GDP increase last year?', 'result': \"\n",
      " \"' The GDP increase last year was 5.7%.'}\")\n"
     ]
    }
   ],
   "source": [
    "# Now try both chains\n",
    "\n",
    "print_with_type(harry_potter.invoke('Why does McGonagall seem concerned about Harry being raised by the Dursleys?'))\n",
    "print_with_type(state_of_union.invoke(\"what is the GDP increase last year?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "73957e1e-f3e2-48e6-91db-d669d5cbe3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, Tool\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"State of Union QA System\",\n",
    "        func=state_of_union.run,\n",
    "        description=\"useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Harry Potter QA System\",\n",
    "        func=harry_potter.run,\n",
    "        description=\"useful for when you need to answer questions about Harry Potter. Input should be a fully formed question.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe451ef-4137-4b86-9254-4117c6802b6a",
   "metadata": {},
   "source": [
    "Now we can create the Agent giving both chains as tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "11b068ff-d822-44ec-ba63-c47f49b492e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "\n",
    "# Construct the agent. We will use the default agent type here.\n",
    "# See documentation for a full list of options.\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b85245e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I should use the State of Union QA System to find the answer\n",
      "Action: State of Union QA System\n",
      "Action Input: What did biden say about ketanji brown jackson?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Biden nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Biden nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Biden nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\n",
    "    \"What did biden say about ketanji brown jackson?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "497fde49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m You should always think about what to do\n",
      "Action: Harry Potter QA System\n",
      "Action Input: 'Why does McGonagall seem concerned about Harry being raised by the Dursleys?'\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m McGonagall is concerned about Harry being raised by the Dursleys because she knows that they have a negative opinion of magic and the wizarding world. She worries that Harry may not be treated well or may not be taught about his magical abilities.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m You should always think about what to do\n",
      "Action: Harry Potter QA System\n",
      "Action Input: 'Why does McGonagall seem concerned about Harry being raised by the Dursleys?'\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m McGonagall is concerned about Harry being raised by the Dursleys because she knows that they have a negative opinion of magic and the wizarding world. She worries that Harry may not be treated well or may not be taught about his magical abilities.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: McGonagall is concerned about Harry being raised by the Dursleys because she knows that they have a negative opinion of magic and the wizarding world. She worries that Harry may not be treated well or may not be taught about his magical abilities.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'McGonagall is concerned about Harry being raised by the Dursleys because she knows that they have a negative opinion of magic and the wizarding world. She worries that Harry may not be treated well or may not be taught about his magical abilities.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\n",
    "    \"'Why does McGonagall seem concerned about Harry being raised by the Dursleys?'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85245e2",
   "metadata": {},
   "source": [
    "We can see that the agent can \"smartly\" choose which QA system to use given a specific question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fde49",
   "metadata": {},
   "source": [
    "## 3 Your Task: putting it all together: OpenAI and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e6d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your Task ####\n",
    "\n",
    "# This is a major task that requires some thinking and time. \n",
    "\n",
    "# Build a conversation system from a collection of research papers of your choice. 5? perhaps\n",
    "# You can ask specific questions of a method about these papers, and the agent returns a brief answer to you (with no more than 100 words).\n",
    " \n",
    "# Save your data and ChromaDB in the /share directory so other people can use it.\n",
    "# Provide at least three query examples so the TAs can review your work. \n",
    "# You may use any tool from the past four labs or from the langchain docs, or any open source project. \n",
    "# write a summary (a Markdown cell) at the end of the notebook summarizing what works and what does not. \n",
    "\n",
    "#basically import some papers, create embeddings for them\n",
    "\n",
    "##Load one pdf - how come arxiv's file does not work\n",
    "data = PyPDFLoader(\"/share/ch3/data.pdf\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08db336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a ChromaDB Embedding\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "minilm_embedding = SentenceTransformerEmbeddings(model_name=\"/share/embedding/all-MiniLM-L12-v2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89e78d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 135 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db09e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_dir = \"/share/ch3/chroma_db\"\n",
    "docsearch_chroma = Chroma.from_documents(texts, \n",
    "                                         minilm_embedding,\n",
    "                                         persist_directory=chroma_dir,\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbb7ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search returned 4 results. \n",
      "be mitigated.\n",
      "5 Implications and Future Directions\n",
      "In this paper, we proposed a metric BARTS CORE that formulates evaluation of generated text as a\n",
      "text generation task, and empirically demonstrated its efficacy. Without the supervision of human\n",
      "judgments, BARTS CORE can effectively evaluate texts from 7 perspectives and achieve the best\n",
      "performance on 16 of 22 settings against existing top-scoring metrics. We highlight potential future\n",
      "directions based on what we have learned.\n",
      "=============\n",
      "BARTS CORE :\n",
      "Evaluating Generated Text as Text Generation\n",
      "Weizhe Yuan\n",
      "Carnegie Mellon University\n",
      "weizhey@cs.cmu.eduGraham Neubig\n",
      "Carnegie Mellon University\n",
      "gneubig@cs.cmu.eduPengfei Liu∗\n",
      "Carnegie Mellon University\n",
      "pliu3@cs.cmu.edu\n",
      "Abstract\n",
      "A wide variety of NLP applications, such as machine translation, summarization,\n",
      "and dialog, involve text generation. One major challenge for these applications\n",
      "is how to evaluate whether such generated texts are actually fluent, accurate,\n",
      "=============\n",
      "By exploring these probabilities, we design metrics that can gauge the quality of the generated text.\n",
      "3.2 BARTScore\n",
      "The most general form of our proposed BARTS CORE is shown in Eq. 2, where we use the weighted\n",
      "log probability of one text ygiven another text x. The weights are used to put different emphasis on\n",
      "different tokens, which can be instantiated using different methods like Inverse Document Frequency\n",
      "(IDF) [25] etc. In our work, we weigh each token equally.2\n",
      "BARTS CORE =mX\n",
      "=============\n",
      "In this paper, we instead argue for a formulation of evaluation of generated text as a text generation\n",
      "problem , directly evaluating text through the lens of its probability of being generated from or generat-\n",
      "ing other textual inputs and outputs. This is a better match with the underlying pre-training tasks and\n",
      "allows us to more fully take advantage of the parameters learned during the pre-training phase. We\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "## Queries first of three: normal\n",
    "docs = docsearch_chroma.similarity_search(\"Who are the authors of the paper 'BARTScore: Evaluating Generated Text as Text Generation'\")\n",
    "print_search_results(docs) ##Find the result after adding the name of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43546a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries second of three: qa chain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=False) ##What are some other chain types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92d822c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The topic of the research paper is evaluating generated text as text generation using a metric called BARTScore.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What is the topic of the research paper BartScore: Evaluating Generated Text as Text Generation'\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f417e76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The paper's experimental result includes an evaluation of the effectiveness of automated scientific reviewing using BARTS CORE, which can evaluate text from various perspectives and estimate measures of quality such as coherence and fluency. It also includes a measure of precision from reference text to system-generated text.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Queries last of three: a different chain\n",
    "chain = load_qa_chain(llm, chain_type=\"map_reduce\", verbose=False)\n",
    "query = \"What does the paper's experimental result that evaluates the effectiveness of automated scientific reviewing include?\" ## also successful##Previous query: What does the paper's experimental result include? ##Previous unsuccessful query: what is hte paper's experimental result?\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89c0051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Need a memory chain for this... it cannot give me an explicit answer\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content = \"\"\"You are an assistent that helps human search for information from research papers.\n",
    "            If you do not know the answer, answer you don't know. Do not try to make up answers.\"\"\"\n",
    "        ),  # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name = \"chat_history\"\n",
    "        ),  # This is where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  # This is where the human input will be injected\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                  input_key=\"human_input\",\n",
    "                                  return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b60a5265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The paper's experimental result includes a fine-grained analysis and prompt analysis, as well as measures such as Kendall's Tau, BERTScore, PRISM, BLEURT, COMET, and BARTS CORE. It also includes evaluations of semantic overlap, linguistic quality, and factual correctness.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=False)\n",
    "query = \"What does the paper's experimental result include?\" ## also successful##Previous query: What does the paper's experimental result include? ##Previous unsuccessful query: what is hte paper's experimental result?\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b566dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The paper's experimental result includes a fine-grained analysis and prompt analysis, as well as measures such as Kendall's Tau, BERTScore, PRISM, BLEURT, COMET, and BARTS CORE. It also includes evaluations of semantic overlap, linguistic quality, and factual correctness.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does the paper's experimental result include?\"\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fcc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##copying from somewhere else\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "chain = load_qa_chain(\n",
    "    OpenAI(temperature=0), chain_type=\"stuff\", memory=memory, prompt=PROMPT\n",
    ")\n",
    "\n",
    "docs=db.similarity_search(query=query)\n",
    "\n",
    "# building the dictionary for chain\n",
    "\n",
    "chain_input={\n",
    "    \"input_documents\": docs,\n",
    "    \"context\":\"This is contextless\",\n",
    "    \"question\":query,\n",
    "    \"Customer_Name\":\"Bob\",\n",
    "    \"Customer_State\":\"NY\",\n",
    "    \"Customer_Gender\":\"Male\"\n",
    "}\n",
    "\n",
    "result=chain(chain_input, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97928714",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Possibly a Tool?\n",
    "from langchain.agents import AgentType, Tool\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"State of Union QA System\",\n",
    "        func=state_of_union.run,\n",
    "        description=\"useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Harry Potter QA System\",\n",
    "        func=harry_potter.run,\n",
    "        description=\"useful for when you need to answer questions about Harry Potter. Input should be a fully formed question.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "harry_potter = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                           chain_type=\"stuff\", \n",
    "                                           retriever=docsearch_chroma_reloaded.as_retriever())\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beb52948",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the chromaDB to local\n",
    "docsearch_chroma.persist() ##how do we know where is it stored?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
