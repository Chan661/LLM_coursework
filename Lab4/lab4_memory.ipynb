{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7731fa-1ab4-48cf-996c-a1a1fb6b2315",
   "metadata": {},
   "source": [
    "# Lec4. Adding Memory and Storage to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b0c75-2dd7-4662-b2ce-4e6b9208926c",
   "metadata": {},
   "source": [
    "Last week, we learned the basic elements of the framework LangChain. In this lecture, we are going to construct a vector store QA application from scratch.\n",
    "\n",
    ">Reference:\n",
    "> 1. [Ask A Book Questions](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/Ask%20A%20Book%20Questions.ipynb)\n",
    "> 2. [Agent Vectorstore](https://python.langchain.com/docs/modules/agents/how_to/agent_vectorstore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a649ab-bb72-4894-b526-c97a7aa1fd81",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34411a5b-ad45-4bf0-bf8e-91f71b256337",
   "metadata": {},
   "source": [
    "1. Install the requirements.  (Already installed in your image.)\n",
    "    ```\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "2. Get your OpenAI API; to get your Serpapi key, please sign up for a free account at the [Serpapi website](https://serpapi.com/); to get your Pinecone key, first regiter on the [Pinecone website](https://www.pinecone.io/), **Create API Key** and **Create Index**. Note that in this notebook the index's dimension should be 1536.\n",
    "\n",
    "3. Store your keys in a file named **.env** and place it in the current path or in a location that can be accessed.\n",
    "    ```\n",
    "    OPENAI_API_KEY='YOUR-OPENAI-API-KEY'\n",
    "    SERPAPI_API_KEY=\"YOUR-SERPAPI-API-KEY\"\n",
    "    PINECONE_API_KEY=\"YOUR-PINECONE-API-KEY\"\n",
    "    PINECONE_API_ENV=\"PINECONE-API-ENV\" # Should be something like \"gcp-starter\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defc9a3a-9f4c-49ff-8546-5799ff78b457",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb49c80a-4c12-4829-bef9-91076a4af689",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159215cd-4035-4f12-9904-e0ad32929b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e349aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edba9d7b-5baa-4769-9c2a-d11164941fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cac64c7-69dc-4450-b2fa-ffd05740411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, PDFMinerLoader\n",
    "\n",
    "data = PyPDFLoader(\"/share/lab4/hp-book1.pdf\").load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a6a49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your TASK ####\n",
    "# Try different PDF Loaders.  Which one works the best for this file /share/lab4/hp-book1.pdf ,\n",
    "# which contains the full book of Harry Potter Book 1, with all the illustratons.\n",
    "\n",
    "## Langchain provides many other options for loaders, read the documents to find out the differences\n",
    "# See page https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
    "loader = UnstructuredPDFLoader(\"./data/field-guide-to-data-science.pdf\")\n",
    "# loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "# loader = PDFMinerLoader(\"example_data/layout-parser-paper.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85245e2",
   "metadata": {},
   "source": [
    "We can see that the agent can \"smartly\" choose which QA system to use given a specific question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fde49",
   "metadata": {},
   "source": [
    "## 3 Your Task: putting it all together: OpenAI and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e6d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your Task ####\n",
    "\n",
    "# This is a major task that requires some thinking and time. \n",
    "\n",
    "# Build a conversation system from a collection of research papers of your choice. 5? perhaps\n",
    "# You can ask specific questions of a method about these papers, and the agent returns a brief answer to you (with no more than 100 words).\n",
    " \n",
    "# Save your data and ChromaDB in the /share directory so other people can use it.\n",
    "# Provide at least three query examples so the TAs can review your work. \n",
    "# You may use any tool from the past four labs or from the langchain docs, or any open source project. \n",
    "# write a summary (a Markdown cell) at the end of the notebook summarizing what works and what does not. \n",
    "\n",
    "#basically import some papers, create embeddings for them\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, PDFMinerLoader\n",
    "##Load one pdf - how come arxiv's file does not work\n",
    "data = PyPDFLoader(\"/share/ch3/data.pdf\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08db336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a ChromaDB Embedding\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "minilm_embedding = SentenceTransformerEmbeddings(model_name=\"/share/embedding/all-MiniLM-L12-v2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89e78d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 135 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db09e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_dir = \"/share/ch3/chroma_db\"\n",
    "docsearch_chroma = Chroma.from_documents(texts, \n",
    "                                         minilm_embedding,\n",
    "                                         persist_directory=chroma_dir,\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbb7ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search returned 4 results. \n",
      "be mitigated.\n",
      "5 Implications and Future Directions\n",
      "In this paper, we proposed a metric BARTS CORE that formulates evaluation of generated text as a\n",
      "text generation task, and empirically demonstrated its efficacy. Without the supervision of human\n",
      "judgments, BARTS CORE can effectively evaluate texts from 7 perspectives and achieve the best\n",
      "performance on 16 of 22 settings against existing top-scoring metrics. We highlight potential future\n",
      "directions based on what we have learned.\n",
      "=============\n",
      "BARTS CORE :\n",
      "Evaluating Generated Text as Text Generation\n",
      "Weizhe Yuan\n",
      "Carnegie Mellon University\n",
      "weizhey@cs.cmu.eduGraham Neubig\n",
      "Carnegie Mellon University\n",
      "gneubig@cs.cmu.eduPengfei Liuâˆ—\n",
      "Carnegie Mellon University\n",
      "pliu3@cs.cmu.edu\n",
      "Abstract\n",
      "A wide variety of NLP applications, such as machine translation, summarization,\n",
      "and dialog, involve text generation. One major challenge for these applications\n",
      "is how to evaluate whether such generated texts are actually fluent, accurate,\n",
      "=============\n",
      "By exploring these probabilities, we design metrics that can gauge the quality of the generated text.\n",
      "3.2 BARTScore\n",
      "The most general form of our proposed BARTS CORE is shown in Eq. 2, where we use the weighted\n",
      "log probability of one text ygiven another text x. The weights are used to put different emphasis on\n",
      "different tokens, which can be instantiated using different methods like Inverse Document Frequency\n",
      "(IDF) [25] etc. In our work, we weigh each token equally.2\n",
      "BARTS CORE =mX\n",
      "=============\n",
      "In this paper, we instead argue for a formulation of evaluation of generated text as a text generation\n",
      "problem , directly evaluating text through the lens of its probability of being generated from or generat-\n",
      "ing other textual inputs and outputs. This is a better match with the underlying pre-training tasks and\n",
      "allows us to more fully take advantage of the parameters learned during the pre-training phase. We\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "## Queries first of three: normal\n",
    "docs = docsearch_chroma.similarity_search(\"Who are the authors of the paper 'BARTScore: Evaluating Generated Text as Text Generation'\")\n",
    "print_search_results(docs) ##Find the result after adding the name of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43546a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries second of three: qa chain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=False) ##What are some other chain types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92d822c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The topic of the research paper is evaluating generated text as text generation using a metric called BARTScore.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What is the topic of the research paper BartScore: Evaluating Generated Text as Text Generation'\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f417e76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The paper's experimental result includes an evaluation of the effectiveness of automated scientific reviewing using BARTS CORE, which can evaluate text from various perspectives and estimate measures of quality such as coherence and fluency. It also includes a measure of precision from reference text to system-generated text.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Queries last of three: a different chain\n",
    "chain = load_qa_chain(llm, chain_type=\"map_reduce\", verbose=False)\n",
    "query = \"What does the paper's experimental result that evaluates the effectiveness of automated scientific reviewing include?\" ## also successful##Previous query: What does the paper's experimental result include? ##Previous unsuccessful query: what is hte paper's experimental result?\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89c0051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Need a memory chain for this... it cannot give me an explicit answer\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content = \"\"\"You are an assistent that helps human search for information from research papers.\n",
    "            If you do not know the answer, answer you don't know. Do not try to make up answers.\"\"\"\n",
    "        ),  # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name = \"chat_history\"\n",
    "        ),  # This is where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  # This is where the human input will be injected\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                  input_key=\"human_input\",\n",
    "                                  return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b60a5265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The paper's experimental result includes a fine-grained analysis and prompt analysis, as well as measures such as Kendall's Tau, BERTScore, PRISM, BLEURT, COMET, and BARTS CORE. It also includes evaluations of semantic overlap, linguistic quality, and factual correctness.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=False)\n",
    "query = \"What does the paper's experimental result include?\" ## also successful##Previous query: What does the paper's experimental result include? ##Previous unsuccessful query: what is hte paper's experimental result?\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b566dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The paper's experimental result includes a fine-grained analysis and prompt analysis, as well as measures such as Kendall's Tau, BERTScore, PRISM, BLEURT, COMET, and BARTS CORE. It also includes evaluations of semantic overlap, linguistic quality, and factual correctness.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does the paper's experimental result include?\"\n",
    "docs = docsearch_chroma.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fcc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "chain = load_qa_chain(\n",
    "    OpenAI(temperature=0), chain_type=\"stuff\", memory=memory, prompt=PROMPT\n",
    ")##copyied from somewhere else\n",
    "\n",
    "docs=db.similarity_search(query=query)\n",
    "\n",
    "# building the dictionary for chain\n",
    "\n",
    "chain_input={\n",
    "    \"input_documents\": docs,\n",
    "    \"context\":\"This is contextless\",\n",
    "    \"question\":query,\n",
    "    \"Customer_Name\":\"Bob\",\n",
    "    \"Customer_State\":\"NY\",\n",
    "    \"Customer_Gender\":\"Male\"\n",
    "}\n",
    "\n",
    "result=chain(chain_input, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97928714",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Possibly a Tool?\n",
    "from langchain.agents import AgentType, Tool\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"State of Union QA System\",\n",
    "        func=state_of_union.run,\n",
    "        description=\"useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Harry Potter QA System\",\n",
    "        func=harry_potter.run,\n",
    "        description=\"useful for when you need to answer questions about Harry Potter. Input should be a fully formed question.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "harry_potter = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                           chain_type=\"stuff\", \n",
    "                                           retriever=docsearch_chroma_reloaded.as_retriever())\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beb52948",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the chromaDB to local\n",
    "docsearch_chroma.persist() ##how do we know where is it stored?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
